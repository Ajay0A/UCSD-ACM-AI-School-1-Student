{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267192c4",
   "metadata": {},
   "source": [
    "# Fashion-MNIST: Feature Engineering + Tiny CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38d82f",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "This notebook walks through a **complete image classification pipeline** on **Fashion-MNIST**:\n",
    "1. Data loading & visualization\n",
    "2. Preprocessing (grayscale already, normalization)\n",
    "3. **Feature engineering** with HOG + Logistic Regression\n",
    "4. **Tiny CNN** baseline (PyTorch)\n",
    "5. Evaluation & comparison (accuracy, confusion matrix)\n",
    "\n",
    "> Runs locally or on Google Colab. If on Colab, enable GPU (Runtime → Change runtime type → GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776164ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# If running in a clean environment, uncomment to install packages:\n",
    "# !pip install torch torchvision torchaudio -q --extra-index-url https://download.pytorch.org/whl/cu126\n",
    "# !pip install scikit-image scikit-learn matplotlib pandas seaborn tqdm ipywidgets notebook-q\n",
    "\n",
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from skimage.feature import hog\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Device selection: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b500a10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "Training samples: 60000, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# 1) Data loading & visualization\n",
    "# Define transformation pipeline: Convert to tensor and normalize\n",
    "# Normalization helps the model converge faster by scaling pixel values to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL Image to tensor and scales [0, 255] to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizes grayscale images: (pixel - 0.5) / 0.5\n",
    "])\n",
    "\n",
    "# Load Fashion-MNIST dataset (28x28 grayscale images of clothing items)\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# 10 classes of clothing items\n",
    "classes = train_dataset.classes\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161eb83",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize sample images from each class\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor idx, class_name in enumerate(classes):\n    # Find first image of this class\n    for i in range(len(train_dataset)):\n        img, label = train_dataset[i]\n        if label == idx:\n            ax = axes[idx // 5, idx % 5]\n            # Denormalize for display: reverse the normalization to get back to [0, 1]\n            ax.imshow(img.squeeze().numpy() * 0.5 + 0.5, cmap=\"gray\")\n            ax.set_title(class_name, fontsize=12)\n            ax.axis(\"off\")\n            break\nplt.tight_layout()\nplt.show()\n\n# Display class distribution in training set\nfrom collections import Counter\ntrain_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\nlabel_counts = Counter(train_labels)\n\nplt.figure(figsize=(12, 4))\nplt.bar([classes[i] for i in range(len(classes))], [label_counts[i] for i in range(len(classes))], color='steelblue')\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Training Set Class Distribution\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "3e805999",
   "metadata": {},
   "source": [
    "### Split a validation set from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d8f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000, Validation samples: 10000, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split training data into train and validation sets\n",
    "# Validation set helps us monitor overfitting during training\n",
    "val_size = 10000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders handle batching, shuffling, and parallel data loading\n",
    "# Shuffle training data to prevent the model from learning the order\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Validation samples: {len(val_ds)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9052084",
   "metadata": {},
   "source": [
    "## Feature Engineering: HOG + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b445058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ea67b5b6484ce2b0d4ca1277846d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting HOG features:   0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3972b0fdce694b4cbe75cc35d0198fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting HOG features:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG feature dimension: 1296\n",
      "\n",
      "HOG+LR Validation Accuracy: 0.8290\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.76      0.80      0.78       400\n",
      "     Trouser       0.95      0.96      0.96       411\n",
      "    Pullover       0.74      0.72      0.73       402\n",
      "       Dress       0.85      0.83      0.84       408\n",
      "        Coat       0.68      0.74      0.71       388\n",
      "      Sandal       0.94      0.92      0.93       380\n",
      "       Shirt       0.57      0.54      0.55       396\n",
      "     Sneaker       0.91      0.91      0.91       411\n",
      "         Bag       0.97      0.92      0.94       407\n",
      "  Ankle boot       0.94      0.94      0.94       397\n",
      "\n",
      "    accuracy                           0.83      4000\n",
      "   macro avg       0.83      0.83      0.83      4000\n",
      "weighted avg       0.83      0.83      0.83      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract HOG (Histogram of Oriented Gradients) features from a subset for speed\n",
    "# HOG captures edge and shape information, which is useful for traditional ML methods\n",
    "N_train = 12000\n",
    "N_val   = 4000\n",
    "\n",
    "def dataset_to_numpy(ds, N):\n",
    "    \"\"\"Convert PyTorch dataset to numpy arrays with HOG features\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in tqdm(range(N), desc=\"Extracting HOG features\"):\n",
    "        img, label = ds[i]\n",
    "        arr = img.squeeze().numpy()  # 28x28 grayscale\n",
    "        # HOG: Extracts gradient orientation histograms from image patches\n",
    "        # pixels_per_cell: size of each cell, cells_per_block: normalization blocks\n",
    "        feat = hog(arr, pixels_per_cell=(4,4), cells_per_block=(2,2), \n",
    "                   orientations=9, block_norm=\"L2-Hys\")\n",
    "        X.append(feat)\n",
    "        y.append(int(label))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "Xtr, ytr = dataset_to_numpy(train_ds, min(N_train, len(train_ds)))\n",
    "Xva, yva = dataset_to_numpy(val_ds,   min(N_val, len(val_ds)))\n",
    "\n",
    "# Standardize features: zero mean, unit variance\n",
    "# This is important for logistic regression to converge properly\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_s = scaler.transform(Xtr)\n",
    "Xva_s = scaler.transform(Xva)\n",
    "\n",
    "print(f\"HOG feature dimension: {Xtr_s.shape[1]}\")\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "clf = LogisticRegression(max_iter=200, n_jobs=-1, verbose=0)\n",
    "clf.fit(Xtr_s, ytr)\n",
    "\n",
    "# Evaluate on validation set\n",
    "pred_va = clf.predict(Xva_s)\n",
    "acc_va = accuracy_score(yva, pred_va)\n",
    "print(f\"\\nHOG+LR Validation Accuracy: {acc_va:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(yva, pred_va, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0717f7",
   "metadata": {},
   "source": [
    "## Tiny CNN Baseline (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a423cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 421642 parameters\n"
     ]
    }
   ],
   "source": [
    "# Define a Tiny CNN (Convolutional Neural Network)\n",
    "# CNNs are designed to automatically learn spatial hierarchies of features from images\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Feature extraction layers\n",
    "        # Conv2d: learns filters to detect edges, textures, and patterns\n",
    "        # ReLU: introduces non-linearity (allows learning complex patterns)\n",
    "        # MaxPool2d: downsamples, reducing spatial dimensions and computation\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # 1 input channel (grayscale), 32 output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),  # 32 input channels, 64 output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
    "        )\n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten 2D feature maps to 1D vector\n",
    "            nn.Linear(64*7*7, 128),  # Fully connected layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),  # Dropout: randomly zeros 50% of neurons during training (prevents overfitting)\n",
    "            nn.Linear(128, num_classes)  # Output layer: 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and move to device (GPU/MPS/CPU)\n",
    "model = TinyCNN().to(device)\n",
    "\n",
    "# Loss function: measures how wrong the predictions are\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: adjusts model weights to minimize loss\n",
    "# Adam is an adaptive learning rate optimizer (works well in practice)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76227fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9472e0ba2e86442099d92f5320accb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ada4995c793480fb791d0e49747e25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.773, Loss: 0.634 | Val Acc: 0.857, Loss: 0.382\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6221e9cb1784ff4aa2984a2437456c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b399ee11406a418ebd1d1bde7aa1c1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.856, Loss: 0.403 | Val Acc: 0.881, Loss: 0.319\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1130471de924829bce95cb70f247feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2b6d7d6ba94d429e92b7f898509ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.877, Loss: 0.347 | Val Acc: 0.893, Loss: 0.286\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d057005f15994c15915df3808d282f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c29f59381894bc39176eaf772c4e2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.887, Loss: 0.317 | Val Acc: 0.900, Loss: 0.266\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d68e168a8e4dc39cd35bcff2e89441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3f04e5a6e24955a3edbcecb75f6f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.897, Loss: 0.287 | Val Acc: 0.906, Loss: 0.249\n"
     ]
    }
   ],
   "source": [
    "# Training loop with progress tracking\n",
    "def run_epoch(loader, train=True):\n",
    "    \"\"\"Run one epoch of training or evaluation\"\"\"\n",
    "    model.train(train)  # Set model to training or evaluation mode\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    \n",
    "    # tqdm creates a progress bar for visual feedback\n",
    "    pbar = tqdm(loader, desc=f\"{'Training' if train else 'Validating'}\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "        \n",
    "        logits = model(x)  # Forward pass: compute predictions\n",
    "        loss = criterion(logits, y)  # Calculate loss\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()  # Backward pass: compute gradients\n",
    "            optimizer.step()  # Update weights\n",
    "        \n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "        \n",
    "        # Update progress bar with current accuracy\n",
    "        pbar.set_postfix({'acc': f'{correct/total:.3f}'})\n",
    "    \n",
    "    return correct/total, loss_sum/total\n",
    "\n",
    "# Train for 5 epochs\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    acc_tr, loss_tr = run_epoch(train_loader, train=True)\n",
    "    acc_va, loss_va = run_epoch(val_loader, train=False)\n",
    "    print(f\"Train Acc: {acc_tr:.3f}, Loss: {loss_tr:.3f} | Val Acc: {acc_va:.3f}, Loss: {loss_va:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9791b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adce54634c44df09a1d59a1900b24ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TINY CNN TEST RESULTS\n",
      "==================================================\n",
      "Test Loss:      0.2627\n",
      "Accuracy:       0.9045  # Proportion of correct predictions\n",
      "Precision:      0.9041  # Of all positive predictions, how many were correct\n",
      "Recall:         0.9045  # Of all actual positives, how many were found\n",
      "F1 Score:       0.9042  # Harmonic mean of precision and recall\n",
      "AUC-ROC:        0.9933  # Area under ROC curve (1.0 = perfect classifier)\n",
      "==================================================\n",
      "\n",
      "Metric Explanations:\n",
      "- Test Loss: How wrong the model's predictions are (lower is better)\n",
      "- Accuracy: Percentage of correctly classified samples\n",
      "- Precision: When model predicts a class, how often is it right?\n",
      "- Recall: Of all samples in a class, how many did we find?\n",
      "- F1 Score: Balance between precision and recall (useful for imbalanced data)\n",
      "- AUC-ROC: Model's ability to distinguish between classes (0.5=random, 1.0=perfect)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set with comprehensive metrics\n",
    "model.eval()\n",
    "y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x = x.to(device)\n",
    "        logits = model(x).cpu()\n",
    "        probs = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "        \n",
    "        y_pred.extend(logits.argmax(1).tolist())\n",
    "        y_true.extend(y.tolist())\n",
    "        y_probs.extend(probs.numpy())\n",
    "\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "acc_test = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# AUC-ROC: measures model's ability to distinguish between classes\n",
    "# Need to binarize labels for multi-class AUC calculation (one-vs-rest)\n",
    "y_true_bin = label_binarize(y_true, classes=list(range(len(classes))))\n",
    "auc_score = roc_auc_score(y_true_bin, y_probs, average='weighted', multi_class='ovr')\n",
    "\n",
    "# Calculate loss on test set\n",
    "test_loss = 0.0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "        total += x.size(0)\n",
    "test_loss /= total\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TINY CNN TEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(f\"Accuracy:       {acc_test:.4f}  # Proportion of correct predictions\")\n",
    "print(f\"Precision:      {precision:.4f}  # Of all positive predictions, how many were correct\")\n",
    "print(f\"Recall:         {recall:.4f}  # Of all actual positives, how many were found\")\n",
    "print(f\"F1 Score:       {f1:.4f}  # Harmonic mean of precision and recall\")\n",
    "print(f\"AUC-ROC:        {auc_score:.4f}  # Area under ROC curve (1.0 = perfect classifier)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Explanation of metrics:\n",
    "print(\"\\nMetric Explanations:\")\n",
    "print(\"- Test Loss: How wrong the model's predictions are (lower is better)\")\n",
    "print(\"- Accuracy: Percentage of correctly classified samples\")\n",
    "print(\"- Precision: When model predicts a class, how often is it right?\")\n",
    "print(\"- Recall: Of all samples in a class, how many did we find?\")\n",
    "print(\"- F1 Score: Balance between precision and recall (useful for imbalanced data)\")\n",
    "print(\"- AUC-ROC: Model's ability to distinguish between classes (0.5=random, 1.0=perfect)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acm-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}